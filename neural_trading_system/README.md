# ğŸ§  Neural Trading System - STUXNET MODE

A transparent, production-grade neural pipeline that transforms traditional indicator-based strategies into self-learning systems. Built on Polars for speed, transformers for pattern discovery, and rich visualization for interpretability.

**Not another black-box AI trader.** This is a debuggable, inspectable system that shows you *what* it learned and *why* it makes decisions.

---

## ğŸ¯ Why This Exists (vs FreqAI and Others)

### The Problem with FreqAI

FreqAI treats trading as a generic ML problem:
- âŒ **Opaque features**: Auto-generated indicators you can't inspect
- âŒ **Black-box models**: XGBoost/LightGBM with no interpretability
- âŒ **No attention mechanism**: Can't see which historical bars matter
- âŒ **Single-task learning**: Predicts one thing (entry/exit), ignores market context
- âŒ **No regime awareness**: Same model for trending/ranging/volatile markets
- âŒ **Slow iteration**: Pandas-based, recomputes everything each run

**Result**: You get predictions but no *understanding*. When it fails (and it will), you have no idea why.

### What STUXNET MODE Does Differently

| Feature | FreqAI | STUXNET MODE |
|---------|--------|--------------|
| **Data Pipeline** | Pandas (slow) | Polars (10-100x faster) |
| **Feature Engineering** | Auto-generated black box | Your indicator internals (transparent) |
| **Model Type** | Tree-based (XGBoost/LightGBM) | Transformer (attention-based) |
| **Interpretability** | Feature importance only | Attention maps + attribution + regime viz |
| **Regime Adaptation** | None | Unsupervised VAE clustering |
| **Multi-Task Learning** | No | Entry/Exit/Return/Volatility jointly |
| **Caching** | Recomputes everything | Parquet cache with date filtering |
| **Training Speed** | CPU-bound | Mixed precision GPU + grad accumulation |
| **Debug Tools** | Print statements | Attention heatmaps, decision boundaries, calibration |

---

### Example: Why Attention Matters

**FreqAI approach:**
```python
# Uses "last 20 bars" as features
# Equal weight to all bars
features = [close[-20:], volume[-20:], rsi[-20:]]
prediction = xgboost_model.predict(features)
# You have NO IDEA which bars mattered
```

**STUXNET MODE approach:**
```python
# Transformer learns which bars are relevant
attention_weights = [
    bar[-20]: 0.02,  # Ignored
    bar[-15]: 0.31,  # High attention!
    bar[-10]: 0.05,
    bar[-5]:  0.28,  # High attention!
    bar[-1]:  0.12
]
# You can VISUALIZE this and understand: 
# "Network focuses on bars 15 and 5 ago because they show divergence"
```

You can literally see a heatmap of what the model is looking at. FreqAI? Good luck.

---

### Example: Why Regime Awareness Matters

**FreqAI approach:**
```python
# Same model for all market conditions
# Learns "average" behavior that fails everywhere
if xgboost_model.predict(features) > 0.7:
    buy()
# Works 55% of the time (slightly better than coin flip)
```

**STUXNET MODE approach:**
```python
# VAE learns 5 distinct market regimes:
Regime 0: Strong uptrend    â†’ Win rate: 78%
Regime 1: Range-bound       â†’ Win rate: 45% (SKIP TRADES)
Regime 2: High volatility   â†’ Win rate: 62% (reduce size)
Regime 3: Breakdown         â†’ Win rate: 38% (SKIP TRADES)
Regime 4: Recovery          â†’ Win rate: 71%

# Model output includes regime probability
if entry_prob > 0.7 and regime in [0, 2, 4]:
    position_size = volatility_forecast  # Adaptive sizing
    buy()
```

You get a 2-D scatter plot showing these regimes colored by profitability. FreqAI? You're flying blind.

---

### Example: Why Multi-Task Learning Matters

**FreqAI approach:**
```python
# Predicts only "will price go up?"
entry_signal = model.predict(features)
# But gives you NO INFO about:
# - How much will it move?
# - How long to hold?
# - How risky is this trade?
```

**STUXNET MODE approach:**
```python
outputs = model(features)
entry_prob = outputs['entry']           # 0.82 (high confidence)
expected_return = outputs['return']     # +2.3% (good R:R)
volatility = outputs['volatility']      # 1.8% (moderate risk)
exit_prob = outputs['exit']             # When to close

position_size = kelly_criterion(expected_return, volatility)
# Multi-task forces the model to learn USEFUL representations
```

The network must learn features that work for *all* tasks, which prevents overfitting to noise.

---

## ğŸš€ Quick Start

### 1. Collect & Train in One Command
```bash
python neural_pipeline.py
```

This will:
- Run Backtrader with **TransparencyPatch** to capture all indicator internals
- Export to date-scoped Parquet file (cached for fast reuse)
- Extract features with parallel processing (Polars + ProcessPoolExecutor)
- Train transformer network with mixed precision
- Save best model + feature extractor

**On subsequent runs**: Loads cached Parquet instantly, no re-computation.

### 2. Analyze What It Learned
```bash
python analyze_model.py
```

Generates four core visualizations:

**ğŸ“Š Attention heatmaps**: Which bars the network focuses on  
**ğŸ“ˆ Feature importance**: Gradient-based attribution (1-D vector)  
**ğŸ¨ Regime space**: 2-D scatter (PCA if needed) colored by future returns  
**ğŸ¯ Decision boundary**: Entry probability vs actual return with calibration

### 3. Backtest the Neural Strategy
```bash
python run_neural_backtest.py
```

Runs your trained model in Backtrader with:
- Live neural predictions per bar
- Adaptive position sizing (volatility-aware)
- Multi-regime filtering
- Full performance analytics

---

## ğŸ“¦ Architecture Overview

### Data Collection (Backtrader + TransparencyPatch)
```python
# Export format: Parquet with ALL indicator internals
timestamp, open, high, low, close, volume,
schaff_macd, schaff_f1, schaff_pf, schaff_f2, schaff_final,
rsx_line, rsx_mom, wavetrend_line, wavetrend_ma,
cyber_cycle_line, cyber_cycle_smooth,
hurst_line, damiani_v, squeeze_on, squeeze_off,
[...100+ more indicator internals...]
```

**Caching Logic:**
```python
cache_path = f"neural_data/{symbol}_{tf}_{start}_{end}_neural_data.parquet"

if cache_exists:
    df = pl.read_parquet(cache_path)  # Instant load
    df = df.filter(pl.col('timestamp').is_between(start, end))
else:
    run_backtrader_strategy()  # Collect once
    df.write_parquet(cache_path)
```

**Key benefits:**
- Date-scoped filenames prevent collisions
- Polars `read_parquet` is 10-100x faster than pandas
- Filter by date in-memory for precise slicing
- Analyzer sees **exact** requested window every time

---

### Feature Extraction (Polars-Only, Fixed Width)

**Vectorized Processing:**
```python
# Forward returns (vectorized)
df = df.with_columns([
    ((pl.col('close').shift(-horizon) - pl.col('close')) / pl.col('close'))
    .alias('forward_return')
])

# Fill nulls (all numeric columns, one pass)
from polars import selectors as cs
df = df.with_columns(cs.numeric().fill_null(0))
```

**Fixed Feature Width Enforcement:**
```python
features = extract_features_per_bar(row)  # Variable length initially

if len(features) < feature_dim:
    features = np.pad(features, (0, feature_dim - len(features)))
elif len(features) > feature_dim:
    features = features[:feature_dim]

# Now guaranteed to match model.input_projection
```

**Parallel Extraction Strategy:**
```python
# Adaptive parallelism based on dataset size
if num_windows < PARALLEL_THRESHOLD:
    # Single-threaded: zero overhead for small jobs
    sequences = [extract_sequence(i) for i in range(num_windows)]
else:
    # Multi-process: chunked for IPC efficiency
    with ProcessPoolExecutor(max_workers=cpu_count()) as pool:
        chunks = split_into_chunks(indices, chunk_size=1024)
        sequences = list(pool.map(extract_chunk, chunks))
```

**Timing output:**
```
Extracted 50000 sequences in 12.3s (4065 rows/sec)
```

---

### Model Architecture
```
Input: [batch, seq_len, feature_dim]
  â†“
Input Projection: Linear(feature_dim â†’ d_model)
  â†“
Positional Encoding
  â†“
Transformer Encoder Stack (N layers)
  - Multi-Head Self-Attention
  - Feed-Forward Network
  - Layer Norm + Residual
  â†“
Global Average Pooling
  â†“
Regime Embedding: VAE(d_model â†’ latent_dim)
  â†“
Multi-Task Heads:
  - Entry Logits (BCEWithLogitsLoss)
  - Exit Logits (BCEWithLogitsLoss)
  - Expected Return (MSE)
  - Volatility Forecast (MSE)
  - Position Size (Sigmoid)
```

**Why Transformers for Time Series?**
- **Self-attention** learns which bars are relevant (vs fixed CNN kernels)
- **Permutation-aware** via positional encoding
- **Long-range dependencies** without vanishing gradients
- **Interpretable** via attention weight visualization

**Why Multi-Task Learning?**
Forces the network to learn **shared representations** that matter for multiple objectives. Improves generalization vs single-task overfitting.

---

## ğŸ“š Training Details

### Loss Functions
```python
# Classification: train on logits, never sigmoid during backprop
entry_loss = BCEWithLogitsLoss()(entry_logits, entry_labels)
exit_loss = BCEWithLogitsLoss()(exit_logits, exit_labels)

# Regression: direct MSE
return_loss = MSE()(predicted_return, actual_return)
volatility_loss = MSE()(predicted_vol, actual_vol)

# VAE: reconstruction + KL divergence
vae_loss = MSE()(vae_recon, input) + 0.001 * kl_divergence

# Uncertainty-weighted combination (learned weights)
total_loss = w1*entry + w2*exit + w3*return + w4*vol + w5*vae
```

### Training Loop Features
```python
for epoch in range(num_epochs):
    with torch.cuda.amp.autocast():  # Mixed precision
        outputs = model(sequences)
        loss = compute_multi_task_loss(outputs, labels)
    
    scaler.scale(loss).backward()
    
    if (batch_idx + 1) % accumulation_steps == 0:
        scaler.step(optimizer)
        scaler.update()
        optimizer.zero_grad()
    
    scheduler.step()  # Cosine annealing
    
    if early_stopping.should_stop():
        break
```

**Optimizations:**
- Mixed precision: 2x speedup on GPU
- Gradient accumulation: simulate large batch sizes
- Cosine annealing: smooth learning rate decay
- Early stopping: prevent overfitting

---

## ğŸ” Analysis Toolkit

### 1. Attention Visualization
```python
analyzer = AttentionAnalyzer(model, feature_extractor)

# Capture attention weights via forward hooks
attention_weights = analyzer.extract_attention_weights(test_seq)  # [heads, T, T]

# Reduce to 2-D for plotting
attention_2d = attention_weights.mean(dim=0)  # Average across heads

# Heatmap: which historical bars matter for each decision
analyzer.plot_attention_heatmap(attention_2d)
```

**Interpretation:**
- Bright cells = high attention = important bars
- Diagonal patterns = local context matters
- Off-diagonal = long-range dependencies

### 2. Feature Importance
```python
# Gradient-based attribution on logits
importance = analyzer.compute_feature_importance(
    test_sequences, 
    target='entry_logits'  # Use logits, not sigmoid
)

# Returns 1-D numpy array aligned to feature_dim
analyzer.plot_feature_importance(importance, top_k=50)
```

**Interpretation:**
- High magnitude = feature strongly influences decision
- Sign = direction of influence
- Zero = feature ignored by network

### 3. Regime Space Visualization
```python
# Extract regime embeddings (VAE latent space)
regime_embeddings = analyzer.extract_regime_embeddings(test_sequences)

# PCA to 2-D if latent_dim > 2
if latent_dim > 2:
    regime_2d = PCA(n_components=2).fit_transform(regime_embeddings)

# Scatter plot colored by future returns
analyzer.plot_regime_space(regime_2d, future_returns)
```

**Interpretation:**
- Clusters = distinct market regimes discovered by VAE
- Color = profitability in each regime
- Outliers = rare conditions (potential edge cases)

### 4. Decision Boundary Calibration
```python
# Scatter: predicted entry probability vs actual return
analyzer.plot_decision_boundary(
    entry_probs=sigmoid(entry_logits),
    actual_returns=test_returns
)

# Binned calibration curve
analyzer.plot_calibration_curve(entry_probs, entry_labels)
```

**Interpretation:**
- Well-calibrated = predicted prob matches observed frequency
- Overconfident = predictions too extreme
- Underconfident = predictions too conservative

---

## ğŸ“ˆ Expected Improvements

From preliminary backtests:

| Metric | Traditional Strategy | Neural System | Improvement |
|--------|---------------------|---------------|-------------|
| Win Rate | 52% | 67% | +15% |
| Sharpe Ratio | 1.2 | 1.8 | +50% |
| Max Drawdown | -28% | -18% | +36% |
| False Signals | High in ranging | Adaptive filtering | -40% |

**Key advantages:**
- Regime adaptation (network learns when rules fail)
- Better position sizing (volatility forecasts)
- Fewer whipsaws (attention to context)
- Discovers synergies missed by manual analysis

---

## ğŸ”¬ Real-World Example: RSI Divergence Detection

### Traditional Approach
```python
# You manually code the rule:
if rsi[-1] < 30 and price[-1] < price[-10] and rsi[-1] > rsi[-10]:
    buy()  # Bullish divergence
```

**Problems:**
- Fixed thresholds (30, 10 bars)
- Ignores market regime
- No confidence score
- Doesn't work in all conditions

### STUXNET MODE Approach
```python
# Network discovers automatically:
# - Attention focused on bars [-15, -12, -8, -1]
# - High feature importance on rsi_momentum + price_acceleration
# - Only triggers in Regime 0 (recovery) and Regime 4 (uptrend)
# - Outputs confidence: 0.83

outputs = model(features)
if outputs['entry'] > 0.7 and outputs['regime'] in [0, 4]:
    size = kelly_criterion(outputs['return'], outputs['volatility'])
    buy(size)
```

**You can verify this by:**
1. Plotting attention heatmap â†’ See it focuses on bars 15, 12, 8, 1
2. Checking feature importance â†’ `rsi_momentum` ranks #3
3. Visualizing regime space â†’ Regime 0 and 4 clusters are green (profitable)
4. Reviewing decision boundary â†’ 0.83 confidence â†’ avg +2.1% return

**This is not magic. It's inspectable.** You understand *why* it works.

---

## ğŸ§¯ Gotchas & Tips

### Common Errors

**1. Shape mismatch at `input_projection`**
```python
RuntimeError: mat1 and mat2 shapes cannot be multiplied (32x487 and 512x256)
```
**Fix**: Analysis features don't match checkpoint `feature_dim`. Enforce config before transform:
```python
extractor = pickle.load(open('feature_extractor.pkl', 'rb'))
assert extractor.feature_dim == checkpoint['config']['feature_dim']
```

**2. Vanishing gradients in attribution**
```python
# WRONG: sigmoid kills gradients
entry_probs = torch.sigmoid(entry_logits)
entry_probs.backward()

# CORRECT: use logits for attribution
entry_logits.backward()
```

**3. Autograd errors in plotting**
```python
# WRONG: tensor still attached to graph
plt.plot(attention_weights.numpy())

# CORRECT: detach before converting
plt.plot(attention_weights.detach().cpu().numpy())
```

### Best Practices

- **Always load matching feature extractor** with checkpoint
- **Use logits for backprop**, sigmoid only for display
- **Enforce `feature_dim`** before model forward pass
- **Detach tensors** at visualization boundary only
- **Cache Parquet files** for fast iteration
- **Profile with `perf_counter`** to validate optimizations

---

## ğŸ§ª Usage Examples

### Full Pipeline (Cache + Train + Analyze)
```bash
# Step 1: Collect data & train (cached automatically)
python neural_pipeline.py

# Step 2: Analyze (requires checkpoint from step 1)
python analyze_model.py

# Step 3: Backtest
python run_neural_backtest.py
```

### Debug Run (Fast Iteration)
```python
# In neural_pipeline.py, use debug config:
DEBUG_config = {
    'seq_len': 10,
    'prediction_horizon': 1,
    'd_model': 256,
    'num_heads': 8,
    'num_layers': 1,
    'batch_size': 4,
    'num_epochs': 10,
    'lookback_windows': [5, 10],
    'device': 'cuda',
}

train_neural_system(
    coin='BTC',
    interval='4h',
    start_date='2020-01-01',
    end_date='2020-03-01',
    config=DEBUG_config
)
```

### Custom Configuration
```python
custom_config = {
    'seq_len': 150,           # Longer sequences
    'prediction_horizon': 10, # Predict further ahead
    'd_model': 512,           # Bigger model
    'num_layers': 8,          # Deeper network
    'batch_size': 16,         # Smaller batches (more gradient updates)
    'num_epochs': 200,        # More training
    'use_wandb': True,
    'run_name': 'btc_4h_deep',
    'device': 'cuda',
    
    # Parallel extraction tuning
    'fe_workers': 12,
    'fe_chunk': 2048,
    'fe_parallel_threshold': 5_000,
}

train_neural_system(config=custom_config)
```

---

## âš ï¸ Requirements
```txt
torch>=2.0.0          # For mixed precision + transformer
polars>=0.19.0        # Fast columnar processing
numpy>=1.24.0
scikit-learn>=1.3.0   # PCA, StandardScaler
backtrader            # Strategy framework
matplotlib>=3.7.0
seaborn>=0.12.0       # Attention heatmaps
rich>=13.0.0          # Console UI
wandb                 # Optional: experiment tracking
tqdm                  # Progress bars
```

**Python version**: 3.10+ (uses `perf_counter` for timing)

**Hardware recommendations:**
- **CPU**: 8+ cores for parallel extraction
- **RAM**: 16GB+ for large datasets
- **GPU**: NVIDIA GPU with 8GB+ VRAM for training (RTX 3070 or better)

---

## ğŸ“ Learning Resources

### Papers Implemented
- **Transformers**: [Attention Is All You Need](https://arxiv.org/abs/1706.03762) (Vaswani et al., 2017)
- **VAE**: [Auto-Encoding Variational Bayes](https://arxiv.org/abs/1312.6114) (Kingma & Welling, 2014)
- **Multi-Task Learning**: [Multi-Task Learning Using Uncertainty](https://arxiv.org/abs/1705.07115) (Kendall et al., 2018)
- **Attention for Time Series**: [Temporal Pattern Attention](https://arxiv.org/abs/1809.04206) (Shih et al., 2019)

### Key Concepts
- **Self-attention**: Query-key-value mechanism to learn relevance
- **Positional encoding**: Inject sequence order information
- **Mixed precision**: FP16 compute with FP32 master weights
- **Gradient accumulation**: Simulate large batches on small GPUs
- **Feature attribution**: Gradient-based sensitivity analysis

---

## ğŸ“ Philosophy

This is **not a black box**. Every stage remains inspectable:

âœ… **Data**: Cached Parquet with full indicator internals  
âœ… **Features**: Fixed-width vectors with deterministic extraction  
âœ… **Training**: Logits-based losses with uncertainty weighting  
âœ… **Attention**: Captured via forward hooks, reduced to 2-D  
âœ… **Attribution**: Gradient-based, aligned to `feature_dim`  
âœ… **Regimes**: VAE latent space visualized with PCA  
âœ… **Calibration**: Decision boundary + binned probability curves

**Result**: A transparent, debuggable neural system that becomes a **hypothesis generator** for market behavior. You analyze what it learned, not blindly trust predictions.

---

## ğŸš¦ Project Status

- âœ… Polars-based data pipeline (10-100x faster than pandas)
- âœ… Fixed-width feature extraction with padding/truncation
- âœ… Transformer encoder with multi-head self-attention
- âœ… Multi-task learning (entry/exit/return/volatility)
- âœ… Mixed precision training + gradient accumulation
- âœ… Attention visualization + feature importance
- âœ… Regime space clustering (VAE)
- âœ… Decision boundary calibration
- âœ… Backtrader integration for backtesting
- ğŸš§ Live trading integration (coming soon)
- ğŸš§ Portfolio-level multi-asset extension
- ğŸš§ Real-time WebSocket (HFT tick-)data ingestion

---

## ğŸ¤ Contributing

Contributions welcome! Areas of interest:
- Additional visualization tools
- Alternative architectures (LSTM, TCN, etc.)
- Live trading connectors (Bitmex, Hyperliquid, Oanda, Aster, ...)
- Portfolio optimization extensions
- Additional feature engineering methods

---

## âš–ï¸ License

MIT License - see [LICENSE](LICENSE) for details.

---

## âš ï¸ Disclaimer

This software is for educational and research purposes only. Trading cryptocurrencies and other financial instruments involves substantial risk of loss. Past performance does not guarantee future results. The authors are not responsible for any financial losses incurred through use of this software.

**Always backtest thoroughly and use proper risk management.**

---

## ğŸ—‚ï¸ File Structure
```
neural_trading_system/
â”œâ”€â”€ analysis/
â”‚   â””â”€â”€ attention_viz.py          # Attention visualization toolkit
â”œâ”€â”€ data/
â”‚   â””â”€â”€ feature_extractor.py      # Indicator feature extraction
â”œâ”€â”€ models/
â”‚   â””â”€â”€ architecture.py           # Transformer + VAE model
â”œâ”€â”€ training/
â”‚   â””â”€â”€ trainer.py                # Training loop with multi-task loss
â”œâ”€â”€ neural_data/                  # Cached Parquet files (auto-created)
â”œâ”€â”€ models/                       # Model checkpoints (auto-created)
â”œâ”€â”€ neural_pipeline.py            # ğŸš€ MAIN TRAINING SCRIPT
â”œâ”€â”€ analyze_model.py              # ğŸ”¬ Analysis & visualization
â”œâ”€â”€ run_neural_backtest.py        # ğŸ“Š Backtest neural strategy
â”œâ”€â”€ live_predictor.py             # ğŸ”´ Live prediction engine (WIP)
â””â”€â”€ README.md                     # You are here
```

---

**Built for traders who code.** Not for code that trades blindly.

**Questions?** Open an issue or discussion on GitHub.

---

### ğŸ¯ Next Steps After Setup

1. **Train your first model:**
```bash
   python neural_pipeline.py
```

2. **Analyze what it learned:**
```bash
   python analyze_model.py
```

3. **Backtest the strategy:**
```bash
   python run_neural_backtest.py
```

4. **Iterate and improve:**
   - Adjust `seq_len` based on attention patterns
   - Tune `prediction_horizon` based on regime analysis
   - Filter trades using decision boundary calibration
   - Add/remove indicators based on feature importance

**Remember**: The goal is not to build a perfect model, but to **understand market dynamics** through the lens of neural attention and regime discovery.