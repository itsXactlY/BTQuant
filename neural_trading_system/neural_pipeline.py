# ==============================================================================
# NEURAL DATA COLLECTION - INTEGRATED WITH YOUR INFRASTRUCTURE
# ==============================================================================

import backtrader as bt
import polars as pl
import torch
import numpy as np
from typing import Dict

# Import your existing infrastructure
from backtrader.TransparencyPatch import activate_patch, capture_patch, export_data, optimized_patch

# Your existing data loader
from backtrader.utils.backtest import PolarsDataLoader, DataSpec


class DataCollectionStrategy(bt.Strategy):
    """
    Strategy that runs ONLY to collect indicator data.
    No trading - pure data harvesting with TransparencyPatch.
    """
    
    params = dict(
        # Enable all indicator blocks
        use_cycle_signals=True,
        use_regime_signals=True,
        use_volatility_signals=True,
        use_momentum_signals=True,
        use_trend_signals=True,
        
        # Indicator parameters from your MegaScalpingStrategy
        cycle_period=20,
        roofing_hp_period=48,
        roofing_ss_period=10,
        hurst_period=100,
        laguerre_length=20,
        damiani_atr_fast=13,
        damiani_std_fast=20,
        damiani_atr_slow=40,
        damiani_std_slow=100,
        damiani_thresh=1.4,
        squeeze_period=20,
        squeeze_mult=2,
        squeeze_period_kc=20,
        squeeze_mult_kc=1.5,
        satr_atr_period=14,
        satr_std_period=20,
        rsx_period=14,
        qqe_period=6,
        qqe_fast=5,
        qqe_q=3.0,
        rmi_period=20,
        rmi_lookback=5,
        wavetrend_period=10,
        kama_period=30,
        kama_fast=2,
        kama_slow=30,
        mesa_fast=20,
        mesa_slow=50,
        ttf_period=15,
        schaff_cycle=10,
        schaff_fast=23,
        schaff_slow=50,
        
        # Control flags
        backtest=True,
        debug=False
    )
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        
        # Initialize ATR (base indicator)
        self.atr = bt.indicators.ATR(self.data, period=14)
        
        # Import all your indicators
        from backtrader.indicators.CyberCycle import CyberCycle
        from backtrader.indicators.ElhersDecyclerOscillator import DecyclerOscillator
        from backtrader.indicators.RoofingFilter import RoofingFilter
        from backtrader.indicators.kama import AdaptiveMovingAverage as KAMA
        from backtrader.indicators.hurst import HurstExponent
        from backtrader.indicators.wavetrend import WaveTrend
        from backtrader.indicators.AdaptiveCyberCycle import AdaptiveCyberCycle
        from backtrader.indicators.AdaptiveLaguerreFilter import AdaptiveLaguerreFilter
        from backtrader.indicators.DamianiVolatmeter import DamianiVolatmeter
        from backtrader.indicators.SqueezeVolatility import SqueezeVolatility
        from backtrader.indicators.StandarizedATR import StandarizedATR
        from backtrader.indicators.RSX import RSX
        from backtrader.indicators.qqe import QQE
        from backtrader.indicators.MesaAdaptiveMovingAverage import MAMA
        from backtrader.indicators.TrendTriggerFactor import TrendTriggerFactor
        from backtrader.indicators.rmi import RelativeMomentumIndex
        from backtrader.indicators.SchaffTrendCycle import SchaffTrendCycle
        
        # Initialize cycle indicators
        if self.p.use_cycle_signals:
            self.cyber_cycle = CyberCycle(self.data, period=self.p.cycle_period)
            self.decycler = DecyclerOscillator(self.data)
            self.roofing = RoofingFilter(
                self.data,
                hp_period=self.p.roofing_hp_period,
                ss_period=self.p.roofing_ss_period
            )
        
        # Initialize regime indicators
        if self.p.use_regime_signals:
            self.hurst = HurstExponent(self.data, period=self.p.hurst_period)
            self.adaptive_cycle = AdaptiveCyberCycle(self.data)
        
        # Initialize volatility indicators
        if self.p.use_volatility_signals:
            self.laguerre = AdaptiveLaguerreFilter(
                self.data,
                length=self.p.laguerre_length
            )
            self.damiani = DamianiVolatmeter(
                self.data,
                atr_fast=self.p.damiani_atr_fast,
                std_fast=self.p.damiani_std_fast,
                atr_slow=self.p.damiani_atr_slow,
                std_slow=self.p.damiani_std_slow,
                thresh=self.p.damiani_thresh
            )
            self.squeeze = SqueezeVolatility(
                self.data,
                period=self.p.squeeze_period,
                mult=self.p.squeeze_mult,
                period_kc=self.p.squeeze_period_kc,
                mult_kc=self.p.squeeze_mult_kc
            )
            self.satr = StandarizedATR(
                self.data,
                atr_period=self.p.satr_atr_period,
                std_period=self.p.satr_std_period
            )
        
        # Initialize momentum indicators
        if self.p.use_momentum_signals:
            self.rsx = RSX(self.data, length=self.p.rsx_period)
            self.qqe = QQE(
                self.data,
                period=self.p.qqe_period,
                fast=self.p.qqe_fast,
                q=self.p.qqe_q
            )
            self.rmi = RelativeMomentumIndex(
                self.data,
                period=self.p.rmi_period,
                lookback=self.p.rmi_lookback
            )
            self.wavetrend = WaveTrend(
                self.data,
                period=self.p.wavetrend_period
            )
        
        # Initialize trend indicators
        if self.p.use_trend_signals:
            self.kama = KAMA(
                self.data,
                period=self.p.kama_period,
                fast=self.p.kama_fast,
                slow=self.p.kama_slow
            )
            self.mesa = MAMA(
                self.data,
                fast=self.p.mesa_fast,
                slow=self.p.mesa_slow
            )
            self.ttf = TrendTriggerFactor(self.data, period=self.p.ttf_period)
            self.schaff = SchaffTrendCycle(
                self.data,
                cycle=self.p.schaff_cycle,
                fast=self.p.schaff_fast,
                slow=self.p.schaff_slow
            )
        
        if self.p.debug:
            print(f"âœ… DataCollectionStrategy initialized with {len(optimized_patch.indicator_registry)} indicators")
    
    def next(self):
        """Capture data every bar - NO TRADING."""
        capture_patch(self)
    
    def stop(self):
        """Called when backtest ends - print summary."""
        if self.p.debug:
            from backtrader.TransparencyPatch import print_patch
            print_patch(auto_export=False)


class NeuralDataPipeline:
    """
    Automated pipeline using YOUR existing infrastructure:
    PolarsDataLoader â†’ Backtrader â†’ TransparencyPatch â†’ Neural Training
    """
    
    def __init__(self, config: Dict):
        self.config = config
        self.loader = PolarsDataLoader()
        
        from data.feature_extractor import IndicatorFeatureExtractor
        self.feature_extractor = IndicatorFeatureExtractor(
            lookback_windows=config.get('lookback_windows', [5, 10, 20, 50, 100])
        )
    
    def collect_data_from_backtrader(
        self,
        coin: str = 'BTC',
        interval: str = '4h',
        start_date: str = '2018-01-01',
        end_date: str = '2024-12-31',
        collateral: str = 'USDT'
    ) -> pl.DataFrame:
        """
        Run backtrader with DataCollectionStrategy to harvest indicator data.
        Uses YOUR existing PolarsDataLoader infrastructure.
        """
        from rich.console import Console
        console = Console()
        
        console.print("ðŸ”¬ [bold cyan]Starting Neural Data Collection[/bold cyan]")
        console.print(f"   Symbol: {coin}/{collateral}")
        console.print(f"   Interval: {interval}")
        console.print(f"   Period: {start_date} â†’ {end_date}")
        
        # Activate transparency patch BEFORE running backtrader
        console.print("\nðŸ”§ [yellow]Activating TransparencyPatch...[/yellow]")
        activate_patch(debug=False)
        
        # Initialize cerebro
        cerebro = bt.Cerebro(oldbuysell=True, runonce=False, stdstats=False)
        
        # Load data using YOUR PolarsDataLoader
        console.print(f"\nðŸ“¥ [cyan]Loading data for {coin}...[/cyan]")
        spec = DataSpec(
            symbol=coin,
            interval=interval,
            start_date=start_date,
            end_date=end_date,
            collateral=collateral
        )
        
        df = self.loader.load_data(spec, use_cache=True)
        data_feed = self.loader.make_backtrader_feed(df, spec)
        
        console.print(f"âœ… [green]Loaded {len(df):,} bars[/green]")
        
        cerebro.adddata(data_feed)
        
        # Add data collection strategy (no trading, just capture)
        cerebro.addstrategy(
            DataCollectionStrategy,
            backtest=True,
            debug=False
        )
        
        # Set minimal broker settings (not needed for data collection, but required)
        cerebro.broker.setcash(10000)
        cerebro.broker.setcommission(commission=0.001)
        
        # Run backtest (pure data collection)
        console.print("\nðŸ“Š [bold green]Running Backtrader to collect indicator data...[/bold green]")
        cerebro.run()
        
        # Export collected data
        console.print("\nðŸ’¾ [yellow]Exporting collected data...[/yellow]")
        df_collected = export_data(
            filename=f'{coin}_{interval}_neural_data',
            export_dir='neural_data'
        )
        
        console.print(f"\nâœ… [bold green]Collection Complete![/bold green]")
        console.print(f"   Bars: {len(df_collected):,}")
        console.print(f"   Features: {len(df_collected.columns)}")
        console.print(f"   OHLCV columns: {len([c for c in df_collected.columns if c in ['open', 'high', 'low', 'close', 'volume']])}")
        console.print(f"   Indicator features: {len([c for c in df_collected.columns if c not in ['bar', 'datetime', 'open', 'high', 'low', 'close', 'volume']])}")
        
        # Clean up
        del cerebro, data_feed
        import gc
        gc.collect()
        
        return df_collected

    def prepare_training_data_optimized(
        self,
        pipeline,
        df: pl.DataFrame,
        prediction_horizon: int = 5
    ) -> Dict:
        """
        Optimized version with batch processing and progress tracking.
        """
        from rich.console import Console
        from tqdm import tqdm
        import multiprocessing as mp
        
        console = Console()
        
        console.print("\nðŸ”§ [cyan]Preparing training data (OPTIMIZED)...[/cyan]")
        
        # Convert to pandas
        df_pd = df.to_pandas()
        
        # Calculate forward returns
        if 'close' not in df_pd.columns:
            raise ValueError("DataFrame must contain 'close' column")
        
        close_prices = df_pd['close'].values
        returns = np.zeros(len(close_prices))
        
        for i in range(len(close_prices) - prediction_horizon):
            returns[i] = (close_prices[i + prediction_horizon] - close_prices[i]) / close_prices[i]
        
        console.print(f"   âœ… Calculated forward returns (horizon={prediction_horizon})")
        
        # Build indicator_data dict
        ohlcv_cols = ['bar', 'datetime', 'open', 'high', 'low', 'close', 'volume']
        indicator_cols = [c for c in df_pd.columns if c not in ohlcv_cols]
        
        console.print(f"   âœ… Found {len(indicator_cols)} indicator features")
        
        # Convert to dict of arrays (faster than repeated DataFrame access)
        indicator_data_full = {}
        for col in df_pd.columns:
            if col not in ['bar', 'datetime']:
                indicator_data_full[col] = df_pd[col].fillna(0).values
        
        # Extract features with progress bar
        seq_len = pipeline.config.get('seq_len', 100)
        console.print(f"\n   Extracting features (seq_len={seq_len})...")
        
        features_list = []
        
        # Batch processing for speed
        batch_size = 100
        total_bars = len(df_pd) - seq_len
        
        with tqdm(total=total_bars, desc="Extracting features") as pbar:
            for i in range(seq_len, len(df_pd)):
                # Build current_data dict (slice up to current bar)
                current_data = {
                    key: values[:i+1] for key, values in indicator_data_full.items()
                }
                
                try:
                    features = pipeline.feature_extractor.extract_all_features(current_data)
                    features_list.append(features)
                except Exception as e:
                    console.print(f"[yellow]Warning at bar {i}: {e}[/yellow]")
                    # Use zeros as fallback
                    if len(features_list) > 0:
                        features_list.append(np.zeros_like(features_list[-1]))
                    else:
                        features_list.append(np.zeros(100, dtype=np.float32))
                
                pbar.update(1)
        
        features = np.array(features_list, dtype=np.float32)
        returns = returns[seq_len:len(features) + seq_len]
        
        console.print(f"\nâœ… [green]Feature extraction complete![/green]")
        console.print(f"   Shape: {features.shape}")
        console.print(f"   Feature dimension: {features.shape[1]}")
        console.print(f"   NaN count: {np.isnan(features).sum()}")
        console.print(f"   Inf count: {np.isinf(features).sum()}")
        
        # Get timestamps
        timestamps = None
        if 'datetime' in df_pd.columns:
            timestamps = df_pd['datetime'].values[seq_len:len(features) + seq_len]
        
        return {
            'features': features,
            'returns': returns,
            'feature_dim': features.shape[1],
            'timestamps': timestamps,
            'indicator_columns': indicator_cols
        }

    def prepare_training_data(
        self,
        df: pl.DataFrame,
        prediction_horizon: int = 5
    ) -> Dict:
        """Use the optimized version."""
        return self.prepare_training_data_optimized(self, df, prediction_horizon)

    def train_neural_model(
        self,
        training_data: Dict,
        save_path: str = 'best_model.pt'
    ):
        """
        Train the neural network on collected data.
        """
        from rich.console import Console
        from torch.utils.data import DataLoader
        from training.trainer import NeuralTrainer, TradingDataset
        from models.architecture import create_model
        
        console = Console()
        console.print("\nðŸ§  [bold magenta]Starting Neural Network Training[/bold magenta]")
        
        features = training_data['features']
        returns = training_data['returns']
        feature_dim = training_data['feature_dim']
        
        # Update config with feature dimension
        self.config['feature_dim'] = feature_dim
        
        # Fit scaler on training data
        train_end = int(len(features) * 0.7)
        console.print(f"\n   Fitting scaler on {train_end:,} training samples...")
        self.feature_extractor.fit_scaler(features[:train_end])
        
        # Normalize features
        console.print("   Normalizing features...")
        features_normalized = np.array([
            self.feature_extractor.transform(f) for f in features
        ])
        
        # Train/val/test split (chronological, no shuffle)
        val_start = int(len(features) * 0.7)
        test_start = int(len(features) * 0.85)
        
        train_features = features_normalized[:val_start]
        train_returns = returns[:val_start]
        
        val_features = features_normalized[val_start:test_start]
        val_returns = returns[val_start:test_start]
        
        test_features = features_normalized[test_start:]
        test_returns = returns[test_start:]
        
        console.print(f"\n   ðŸ“Š Data Split:")
        console.print(f"      Train: {len(train_features):>8,} bars ({len(train_features)/len(features)*100:>5.1f}%)")
        console.print(f"      Val:   {len(val_features):>8,} bars ({len(val_features)/len(features)*100:>5.1f}%)")
        console.print(f"      Test:  {len(test_features):>8,} bars ({len(test_features)/len(features)*100:>5.1f}%)")
        
        # Create datasets
        train_dataset = TradingDataset(
            train_features, train_returns,
            seq_len=self.config['seq_len'],
            prediction_horizon=self.config.get('prediction_horizon', 5)
        )
        
        val_dataset = TradingDataset(
            val_features, val_returns,
            seq_len=self.config['seq_len'],
            prediction_horizon=self.config.get('prediction_horizon', 5)
        )
        
        # Create dataloaders
        train_loader = DataLoader(
            train_dataset,
            batch_size=self.config.get('batch_size', 32),
            shuffle=False,  # Don't shuffle time series!
            num_workers=4,
            pin_memory=True if torch.cuda.is_available() else False
        )
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=self.config.get('batch_size', 32),
            shuffle=False,
            num_workers=4,
            pin_memory=True if torch.cuda.is_available() else False
        )
        
        # Create model
        console.print("\nðŸ—ï¸  [cyan]Building neural architecture...[/cyan]")
        model = create_model(feature_dim, self.config)
        
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        console.print(f"   Total parameters:     {total_params:>12,}")
        console.print(f"   Trainable parameters: {trainable_params:>12,}")
        
        # Create trainer
        trainer = NeuralTrainer(
            model=model,
            train_loader=train_loader,
            val_loader=val_loader,
            config=self.config,
            device=self.config.get('device', 'cuda' if torch.cuda.is_available() else 'cpu')
        )
        
        # Train
        console.print("\nðŸŽ¯ [bold green]Starting training loop...[/bold green]")
        trainer.train(self.config.get('num_epochs', 100))
        
        # Save feature extractor
        import pickle
        feature_extractor_path = save_path.replace('.pt', '_feature_extractor.pkl')
        with open(feature_extractor_path, 'wb') as f:
            pickle.dump(self.feature_extractor, f)
        
        console.print(f"\nâœ… [bold green]Training complete![/bold green]")
        console.print(f"   Model saved to: {save_path}")
        console.print(f"   Feature extractor saved to: {feature_extractor_path}")
        
        return trainer, test_features, test_returns


# ==============================================================================
# ONE-COMMAND TRAINING PIPELINE
# ==============================================================================

def train_neural_system(
    coin: str = 'BTC',
    interval: str = '4h',
    start_date: str = '2018-01-01',
    end_date: str = '2024-12-31',
    collateral: str = 'USDT',
    config: Dict = None
):
    """
    ðŸš€ ONE FUNCTION TO RULE THEM ALL
    
    Uses YOUR existing infrastructure:
    - PolarsDataLoader for data loading with caching
    - TransparencyPatch for indicator capture
    - Neural network for pattern learning
    
    Example:
        >>> train_neural_system(
        ...     coin='BTC',
        ...     interval='4h',
        ...     start_date='2020-01-01',
        ...     end_date='2024-12-31'
        ... )
    """
    from rich.console import Console
    from rich.panel import Panel
    
    console = Console()
    
    # Default config
    if config is None:
        config = {
            # Data collection
            'seq_len': 100,
            'prediction_horizon': 5,
            'lookback_windows': [5, 10, 20, 50, 100],
            
            # Model architecture
            'd_model': 256,
            'num_heads': 8,
            'num_layers': 6,
            'd_ff': 1024,
            'dropout': 0.1,
            'latent_dim': 8,
            
            # Training
            'batch_size': 32,
            'num_epochs': 100,
            'lr': 1e-4,
            'min_lr': 1e-6,
            'weight_decay': 1e-5,
            'grad_accum_steps': 4,
            'T_0': 10,
            'patience': 15,
            'save_every': 10,
            
            # Experiment tracking
            'use_wandb': True,
            'run_name': f'neural_{coin}_{interval}',
            'device': 'cuda' if torch.cuda.is_available() else 'cpu'
        }
    
    console.print(Panel.fit(
        f"[bold cyan]NEURAL TRADING SYSTEM[/bold cyan]\n"
        f"[yellow]Automated Training Pipeline[/yellow]\n\n"
        f"Symbol: {coin}/{collateral}\n"
        f"Interval: {interval}\n"
        f"Period: {start_date} â†’ {end_date}\n"
        f"Device: {config['device']}",
        title="ðŸ§  Configuration",
        border_style="cyan"
    ))
    
    # Initialize pipeline
    pipeline = NeuralDataPipeline(config)
    
    # Step 1: Collect data from Backtrader (with TransparencyPatch)
    df = pipeline.collect_data_from_backtrader(
        coin=coin,
        interval=interval,
        start_date=start_date,
        end_date=end_date,
        collateral=collateral
    )
    
    # Step 2: Prepare training data
    training_data = pipeline.prepare_training_data(
        df,
        prediction_horizon=config['prediction_horizon']
    )
    
    # Step 3: Train neural model
    model_path = f'models/neural_{coin}_{interval}_{start_date}_{end_date}.pt'
    trainer, test_features, test_returns = pipeline.train_neural_model(
        training_data,
        save_path=model_path
    )
    
    console.print("\n" + "="*80)
    console.print(Panel.fit(
        "[bold green]âœ… PIPELINE COMPLETE![/bold green]\n\n"
        f"Model: {model_path}\n"
        f"Feature Extractor: {model_path.replace('.pt', '_feature_extractor.pkl')}\n\n"
        "[yellow]Next steps:[/yellow]\n"
        "  1. Analyze what the network learned:\n"
        "     [cyan]python analyze_model.py[/cyan]\n"
        "  2. Run backtest with neural strategy:\n"
        "     [cyan]python run_neural_backtest.py[/cyan]\n"
        "  3. Deploy to live trading:\n"
        "     [cyan]python deploy_live.py[/cyan]",
        title="ðŸŽ‰ Success",
        border_style="green"
    ))
    
    return pipeline, trainer, training_data


# ==============================================================================
# USAGE EXAMPLES
# ==============================================================================

if __name__ == '__main__':
    
    # Example 1: Train on BTC 4h data
    pipeline, trainer, data = train_neural_system(
        coin='BTC',
        interval='4h',
        start_date='2020-01-01',
        end_date='2024-12-31',
        collateral='USDT'
    )
    
    # Example 2: Custom configuration
    custom_config = {
        'seq_len': 150,  # Longer sequences
        'prediction_horizon': 10,  # Predict further ahead
        'd_model': 512,  # Bigger model
        'num_layers': 8,  # Deeper network
        'batch_size': 16,  # Smaller batches
        'num_epochs': 200,  # More training
        'use_wandb': True,
        'run_name': 'btc_4h_deep',
        'device': 'cuda'
    }
    
    # pipeline, trainer, data = train_neural_system(
    #     coin='BTC',
    #     interval='4h',
    #     start_date='2020-01-01',
    #     end_date='2024-12-31',
    #     config=custom_config
    # )
    
    # Example 3: Train on multiple timeframes (run separately)
    # for interval in ['1h', '4h', '1d']:
    #     train_neural_system(
    #         coin='BTC',
    #         interval=interval,
    #         start_date='2020-01-01',
    #         end_date='2024-12-31'
    #     )